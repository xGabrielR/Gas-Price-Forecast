{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13ebb1e0-311e-4713-8d62-b2b00f6e0eb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Gas Price Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f049b9c0-a808-48a1-a508-9ece82b123d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "My Organization in Lake (You can organize as you want):\n",
    "\n",
    "1. bucket_name is the bucket name of AWS.\n",
    "2. date_file is a format of storing daily, monthly or weekly... data to landing, e.g: 20230101.\n",
    "\n",
    "- s3://{bucket_name}/{date_file}/gas/ * .csv\n",
    "- s3://{bucket_name}/{date_file}/eletricity/ * .json\n",
    "- s3://{bucket_name}/{date_file}/public_transport/ *.parquet\n",
    "\n",
    "Now at data lake / obj storage i can store to bronze in lakehouse for next improvements and sequences in lakehouse.\n",
    "\n",
    "Now for each source i will prepare the dataset to store on bronze layer of lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ada6dfc-9d80-4b56-a47c-6fea3dffd254",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 0.0. Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0270149b-7ccd-425d-b1a3-56c13721c29f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 0.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9994c34a-4a58-47dc-8150-0463ee0fb432",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import functions as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edb50649-3171-4ea3-97bb-9d7c5b01ec5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 0.2. Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbbbbcbb-c659-483c-9280-e8b41b5880f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_insert_mode(table_name: str):\n",
    "    excep = \"None\"\n",
    "\n",
    "    if table_name == \"gas\":\n",
    "        insert_mode = \"append\"\n",
    "        raw_file_format = \"csv\"\n",
    "        date_file = datetime.now(tz=timezone(\"America/Sao_Paulo\")).strftime(\"%Y%m%d\")\n",
    "\n",
    "    elif table_name == \"eletricity\":\n",
    "        pass\n",
    "\n",
    "    return excep, date_file, insert_mode, raw_file_format\n",
    "\n",
    "\n",
    "def read_file_format(\n",
    "    file_format: str,\n",
    "    bucket_name: str,\n",
    "    table_name: str,\n",
    "    date_file: str\n",
    "):\n",
    "    try:\n",
    "        if file_format == \"csv\":\n",
    "            df = spark.read.format(file_format) \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .load(f\"s3a://{bucket_name}/{date_file}/{table_name}/*.{file_format}\")\n",
    "\n",
    "        elif file_format == \"json\":\n",
    "            df = spark.read.format(file_format) \\\n",
    "                    .option(\"multiLine\", \"true\") \\\n",
    "                    .load(f\"s3a://{bucket_name}/{date_file}/{table_name}/*.{file_format}\")\n",
    "\n",
    "        else:\n",
    "            return \"FORMAT OR PATH_NOT_FOUND\"\n",
    "\n",
    "    except Exception as e:\n",
    "        excep = e.desc\n",
    "\n",
    "        return excep\n",
    "    \n",
    "\n",
    "def write_to_bronze(\n",
    "    load_object\n",
    "):\n",
    "    df = df.withColumn(\"data_base\", pf.lit(date_file))\n",
    "\n",
    "    if insert_mode == \"append\":\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(insert_mode) \\\n",
    "            .partitionBy(\"data_base\") \\\n",
    "            .save(f\"s3://{bucket_name}/gas_price_forecast/{table_name}\")\n",
    "\n",
    "    elif insert_mode == \"overwrite\":\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(insert_mode) \\\n",
    "            .partitionBy(\"data_base\") \\\n",
    "            .save(f\"s3://{bucket_name}/gas_price_forecast/{table_name}\")\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "def ingest_to_bronze(\n",
    "    table_name: str,\n",
    "    landing_bucket_name: str,\n",
    "    bronze_bucket_name: str\n",
    "):\n",
    "    # Get Table Name Information\n",
    "    excep, date_file, insert_mode, file_format = get_insert_mode(table_name)\n",
    "\n",
    "    s = \"\"\n",
    "    s = f'Start: {datetime.now(tz=timezone(\"America/Sao_Paulo\"))}'\n",
    "    s += f\"\\n{'=' * 39}\\n\"\n",
    "    s += \"File: {}\\nDateCut: {}\\nInsertMode: {}\\nFileFormat: {}\".format(table_name, date_file, insert_mode, file_format)\n",
    "    s += f\"\\n{'=' * 39}\\n\"\n",
    "\n",
    "    # Load From Bronze\n",
    "    load_object  =read_file_format(file_format, landing_bucket_name, table_name, date_file)\n",
    "\n",
    "    # Ingest to Bronze\n",
    "    if type(load_object) == str:\n",
    "        print(load_object)\n",
    "\n",
    "    else:\n",
    "        write_to_bronze(load_object)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d82ebc14-5b71-4c22-a825-ac7c42bb680d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 0.3. S3 Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa8e51a-ec59-4152-a680-b5abcd91c091",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# secrets is not available on Community =D\n",
    "#access_key = dbutils.secrets.get(scope = \"aws\", key = \"aws-access-key\")\n",
    "#secret_key = dbutils.secrets.get(scope = \"aws\", key = \"aws-secret-key\")\n",
    "#landing_bucket_name = dbutils.secrets.get(scope = \"aws\", key = \"aws-bucket-landing\")\n",
    "#bronze_bucket_name = dbutils.secrets.get(scope = \"aws\", key = \"aws-bucket-bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd6fdee-f497-43e7-8148-380e0f4d471f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# encoded_secret_key = secret_key.replace(\"/\", \"%2F\")\n",
    "# aws_bucket_name = \"example-aws-bucket\"\n",
    "# mount_name = \"mount-name\"\n",
    "# \n",
    "# dbutils.fs.mount(\n",
    "#     f\"s3a://{access_key}:{encoded_secret_key}@{aws_bucket_name}\", \n",
    "#     f\"/mnt/{mount_name}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e8ef518-a77a-4a1d-b685-ab56822714ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"CREATE EXTERNAL LOCATION [IF NOT EXISTS] <example-location-name-aws> URL 's3://<bucket-name>/<example-path>'\n",
    "#     WITH (CREDENTIAL <aws-credential-name>)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "349cdd70-bc53-4138-8139-c947acbfb401",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Another Way\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"3\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.disable.chunked.encoding\", \"True\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.path.style.access\", \"True\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f0fae86-5922-4657-ade3-8cbce253d07d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1.0. Load to Bronze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59db04c6-1f75-49fc-8cb0-6c9cad2686c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 1.1. Gas Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe4faad-1bb8-4d33-8547-c903a087f05e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"gas\"\n",
    "excep, date_file, insert_mode, file_format = get_insert_mode(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e98dbda-84b1-4fb1-965e-0f618e97f65c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid URI: null or empty\n"
     ]
    }
   ],
   "source": [
    "ingest_to_bronze(\n",
    "    table_name,\n",
    "    landing_bucket_name,\n",
    "    bronze_bucket_name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "grc_bronze_ingestion",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
